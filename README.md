# ComputeGraph Homework


В этом задании вы продолжите работать над библиотекой для вычислений над графами.

Напомним определение таблицы из предыдущей домашки:

```
Таблица - это последовательность словарей, где каждый словарь — это строка таблицы, а ключ словаря — колонка таблицы
(индекс в последовательности + ключ в словаре задают ячейку).
```

Над таблицами мы будем запускать вычисления, задавая их с помощью *вычислительных графов*.

Под вычислительным графом мы будем понимать заранее заданную последовательность операций, которую затем можно применять к
различным наборам данных.

Для простоты можно считать, что все строки во входных таблицах содержат одинаковый набор ключей.
При этом вашим операциям не запрещается создавать строки, у которых для некоторых ключей значения не определены, например:
`{'key1': 1, 'key2': None}`, но в этом случае необходимо сделать реализацию всех операций устойчивой к такому поведению.

## Зачем вообще нужны вычислительные графы?

Вычислительные графы позволяют отделить описание последовательности операций от их выполнения. Благодаря этому, вы можете как
запускать операции в другой среде (например, описать граф в интерпретаторе питона, а затем выполнить на видеокарте),
так и независимо и параллельно запускать на множестве машин вычислительного кластера для обработки большого массива
входных данных за адекватное конечное время (например, так работает клиент к системе распределенных вычислений Spark).

Второй пример связан с уже знакомыми вам операциями `Map` / `Reduce`, и реализацией подобного графа мы и займемся
с некоторыми оговорками:
* Будем выполнять код прямо на ваших компьютерах (это же учебное задание, в конце концов)
* Представим, что данные потенциально могут не влезать целиком в память (но все же их объем конечен*)
* Будем обращать внимание на производительность нашего решения (об этом ниже)


[*] Для бесконечного объема входных данных конечное время работы не гарантировано :)

## Интерфейс графа вычислений

Граф вычислений состоит из точек входа для данных и операций над ними.

Вот так может выглядеть (хотя кого я пытаюсь обмануть, так и есть - см. [`graphs.py`](graphs.py)) граф,
который подсчитывает кол-во слов в документах:
```python
graph = Graph() \
    .read_from_iter('texts') \
    .map(operations.FilterPunctuation('text')) \
    .map(operations.LowerCase('text')) \
    .map(operations.Split('text')) \
    .sort(['text']) \
    .reduce(operations.Count('count'), ['text']) \
    .sort(['count', 'text'])
```

Ещё раз заострим внимание: в момент создания графа мы не производим никаких чтений данных и вычислений.

Обратите внимание на интерфейс графа, каждая операция задается вызовом соответствующего метода класса `Graph`
(полный список операций см. в [`lib/graph.py`](lib/graph.py)) - это рекомендуемая реализация, которая тем не менее **не является
обязательной**. Вы можете менять интерфейс графа на своё усмотрение (единственное, что требуется - тесты должны
работать как есть, без изменений).

Входные данные могут подаваться как в виде имен файлов с таблицами, которые необходимо открыть и прочитать,
так и в виде произвольных генераторов, которые возвращают по одной строке за раз (так сделано в
тестах, за примерами обращайтесь туда).

Также вы **можете** менять внутреннюю структуру библиотеки. Например, файл с операциями сейчас - это полная копия
заготовки из предыдущего семинарского задания. Такая структура вполне может оказаться неудобной для вас, поэтому
мы не стали завязываться в явном виде на ваш код прошлого задания, и вы можете как полностью скопировать его сюда,
так и переписать.

Таблицы в файлах хранятся в виде последовательностей json-словарей (по одному на строку), каждый из которых описывает
одну строку итоговой таблицы (см. директорию [`resource`](resource)).

Таблицы, получаемые из генераторов нам показалось удобным реализовать так, чтобы в самом графе указывалось лишь ключевое
слово для идентификации источника данных, тогда как сами данные передаются непосредственно во время запуска:
```python
dummy_graph = Graph().read_from_iter(name='docs')

graph.run(docs=[{'key': 'value'}])
```

Создание графов над файлами мы не стали покрывать тестами, однако предоставляем вам набор данных, на которых вы должны
самостоятельно протестировать ваш код (это обязательное условие сдачи, см. ниже).

Над входными данным мы будем запускать всё те же знакомые по предыдущему заданию операции.
В общем случае операции вызываются последовательно, но есть операции (`join`), которые на вход принимают другие
графы вычислений; за счет этого полный граф вычислений может быть нелинеен.

**Очень важный момент**: нелинейность в графе может возникнуть и без `join`'а:
```python
graph = Graph().operation1(...).operation2(...)

# Caution! Non-linear execution flow
graph1 = graph.operation3()
graph2 = graph.operation4()

# Pure evil (he-he-he)
final_graph = graph1.join(..., graph2, ...)
```
И да, вы должны уметь обрабатывать такое поведение в своем коде.

После описания граф нужно уметь запускать, передав ему все нужные входные данные, каким-нибудь
методом `run` (ладно-ладно, именно таким, это зашито в тестах):
```python
def run(self, **kwargs) -> List[Row]:
    pass
```
Обратите внимание, что однажды созданный граф **нужно уметь запускать на разных входах без пересоздания**
(и мы это проверяем в тестах [в одном, да, подловили]).

## Требования к сдаче

1. Реализовать ваш вариант библиотеки для создания и использования вычислительных графов.
Подумать над существующим интерфейсом, его можно менять, но тесты должны работать.
Ограничения на реализацию:
    * Последовательные запуски не должны влиять друг на друга
    * Результаты запусков должны быть воспроизводимы
    * Вычисления должны запускаться на вашем же компьютере (мы лишь имитируем настоящий MapReduce)
    * Данные потенциально могут не влезать целиком в память (используйте генераторы)
    * Учитывается производительность решения (не стоит прокручивать всю таблицу последовательно
    одним редьюсером, если можно разбить таблицу по ключу и редьюсить каждый кусок независимо;
    честно параллелить вызовы редьюсеров не требуется)

2. Для библиотеки написать юнит-тесты на pytest (hint: тесты на операции можно утащить из прошлой домашки),
и положить их **внутрь** библиотеки (не стоит их класть в `test_public.py`, давайте разделять библиотеку, и задачи,
которые можно с её помощью решать). 

3. Снабдить весь (!) код docstring-ами (правильный формат dostring-ов описан в
[PEP-0257](https://www.python.org/dev/peps/pep-0257/)). За массовое отсутствие докстрингов можем снижать баллы.

4. Используя вашу библиотеку решить 4 задачи, приведенные ниже.
Задача должна целиком решаться в модели вычислительного графа - от чтения входа до записи результата.
Граф, решающий задачу положить в `graphs.py` (первый идет бонусом, можно переделать под себя).
Выполнение задач на простых входных данных контролируют наши тесты. Формальным критерием сдачи является прохождение
тестов (если ваше задание содержит не все тесты, мы оставляем за собой право не принять код на ревью).

5. Для каждой задачи сделайте тест, позволяющий быстро запустить её на входных данных из `resource`.
Оформите это как отдельные примеры использования вашей библиотеки и положите рядом.
Отдельным жирным бонусом будет какая-нибудь визуализация для последней задачи (но это по желанию). 

6. Задача проверяется на code review. Задача не будет зачтена, пока ревьювер не скажет, что она зачтена.
Мы гарантируем всем 1 итерацию ревью (а именно, первичное вычивание и комментирование кода, а также
повторное чтение с резюмированием и вынесением балла), можем сделать и больше по своему усмотрению,
но только в том случае, если вы пришлете код не под дедлайн.

Тесты, документированность и порядок в директории с проектом оцениваются **наравне** с качеством решения задач. 

## Задачи для решения с помощью вашей библиотеки

Во всех задачах формат результата должен соответствовать тестам.

### Word Count

Задача для разминки (базовый граф уже сделан, можно переделать по желанию).

В этой задаче вам дана таблица со строками в формате `{'doc_id': ..., 'text :...'}`.
Требуется для каждого из слов, встречающихся в колонке `text`, посчитать количество вхождений во всю таблицу в сумме.

Файл с данными для этой и двух следующих задач: [`resource/text_corpus.txt`](resource/text_corpus.txt)

### Инвертированный индекс на tf-idf

Работать будем с той же таблицей, что и в предыдущей задаче.

Для этой коллекции построим *инвертированный индекс* — структуру данных, которая для каждого слова хранит
список документов, в котором оно встречается, отсортированный в порядке *релевантности*.

Релевантность будем считать по метрике [tf-idf](https://ru.wikipedia.org/wiki/TF-IDF).

Для каждой пары (слово, документ) tf-idf зададим так:
```
TFIDF(word_i, doc_i) = (frequency of word_i in doc_i) * log((total number of docs) / (docs where word_i is present))
```

Для каждого слова надо посчитать топ-3 документов по tf-idf.

Когда будете делить текст на слова, не забудьте убрать пунктуацию (аналогично предыдущей задаче).

#### Подробный разбор задачи

На входе нам дана таблица с колонками `doc_id`, `text`.

На выходе для каждого слова из исходных документов хотим получить последовательность `doc_id`
в порядке убывания tf-idf для этого слова в этом документе.

Будем решать поэтапно:

1. Создадим промежуточный граф, который пропустит вход нашей исходной таблички через `mapper`, который разделит
каждую строку с текстом на слова.
    ```
    split_word := input('docs') -> map(split_words)
    ```

2. Создадим еще один граф, который посчитает количество документов в таблице (оно нужно нам для формулы idf).
Он будет состоять из единственной операции `reduce`, которая будет считать количество строчек в переданной ей таблице.
    ```
    count_docs := input('docs') -> reduce(count_rows)
    ```

3. Теперь опишем граф, считающий idf каждого слова. Входом для этого графа будет выход графа из п.1.
Для начала запустим на этой таблице reduce по ключу `('doc_id', 'word')` и для каждого ключа оставим только одно его
вхождение (ведь нас интересует количество документов в которых встретилось слово, без разницы сколько раз).
Результат этой операции отсортируем и будем редьюсить уже по словам (т. е. по `word`), считая количество документов,
в которых это слово встретилось. Далее сджойним граф из п.2 с нашей таблицей, используя `inner join`,
фактически дописав в каждую строку таблицы общее кол-во текстов. Полученную тапличу прогонним через маппер, который
посчитает для каждой строки idf, используя имеющую информацию о кол-ве документов, содержащих слово, и суммарном кол-ве
документов.
   ```
   count_idf := input(split_words) -> sort('doc_id', 'word') ->
        -> reduce(first, keys=('doc_id', 'word')) -> sort('word') ->
        -> join(inner, count_docs) -> map(idf)
   ```

4. Создадим ещё граф. Входом для него так же, как для предыдущего графа, будет результат графа из п.1,
но в этот раз мы его будем редьюсить по `doc_id`, считая частоту каждого слова, то есть числитель нужной нам формулы.
   ```
   tf := input(split_word) -> sort('doc_id') -> reduce(tf, 'doc_id')
   ```

5. После этого нужно сделать join c результатом графа из п.3 и перемножить соответствующие метрики.
Последним будет reduce по `word`, который выберет для каждого слова top-3 документа по tf-idf.

### Топ слов с наибольшей взаимной информацией

Задача, обратная предыдущей: для каждого документа посчитать топ-10 слов, наиболее характерных для него.

Ранжировать слова будем по метрике
[Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information).

Более формально задача ставится так: для каждого текста надо найти топ-10 слов, каждое из которых длиннее четырех
символов и встречается в каждом из документов не менее двух раз; топ надо выбирать по величине:
```
pmi(word_i, doc_i) = log((frequency of word_i in doc_i) / (frequency of word_i in all documents combined))
```

**Upd.** Если у вас не сходятся результаты расчетов с тестами, то обратите внимание, что слова, не соответствующие
условию выкидываются ещё **до** расчетов частот. Воспринимайте это как "фичу" :)

### Средняя скорость движения по городу от часа и дня недели

В этой задаче вам надо работать с информацией о движении людей на машинах по какому-то подмножеству улиц города Москвы.

Улицы города заданы как граф, а информация о передвижении задана как таблица, в каждой строке которой данные вида
```
{'edge_id': '624', 'enter_time': '20170912T123410.1794', 'leave_time': '20170912T123412.68'}
```
где `edge_id` — идентификатор ребра графа дорог (то есть просто участка какой-то улицы), а `enter_time` и `leave_time` —
соответственно время въезда и выезда с/на это ребро (время в UTC).

Также вам дана вспомогательная таблица вида
```
{'edge_id': '313', 'length':121, 'start': [37.31245, 51.256734], 'end': [37.31245, 51.256734]}
```
где `length` - длина в метрах, `start` и `end` — координаты начала и конца ребра, заданные в формате `('lon', 'lat')`.
Быть может, не для всех рёбер графа есть вся метаинформация.

Пользуясь этой информацией вам надо построить таблицу со средней скоростью движения по городу в км/ч
в зависимости от часа и дня недели:
```
{'weekday': 'Mon', 'hour': 4, 'speed': 44.812}
```

Для проверки полезно построить график по этой таблице, он должен выглядеть предсказуемо (если покажете нам, дадим бонусные баллы).

Файлы для этой задачи: [`resource/travel_times.txt`](resource/travel_times.txt)
и [`resource/road_graph_data.txt`](resource/road_graph_data.txt) 

## Замечания
  1.  Для того, чтобы посчитать в каком порядке выполнять все графы, от которых зависит данный, потребуется
  [Топологическая сортировка](https://ru.wikipedia.org/wiki/%D0%A2%D0%BE%D0%BF%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%81%D0%BE%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0).
  Это не сложный алгоритм, разберитесь.
  2. Приватных тестов в домашке нет (нам нечего скрывать!)
